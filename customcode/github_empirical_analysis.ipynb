{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from parser import *\n",
    "from snapshoter import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from ruamel.yaml import YAML"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the dataset and filtering the deleted workflows",
   "id": "d0f477814f578a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T17:03:18.384560Z",
     "start_time": "2025-03-31T17:03:07.206931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('../dataset/200_workflowsonly.csv')\n",
    "print(df.shape[0])\n",
    "\n",
    "deleted_workflows = df.loc[df['file_hash'].isnull()]\n",
    "print(f\"Number of deleted workflows records : {len(deleted_workflows)}\")\n",
    "\n",
    "df = df.dropna(subset=['file_hash'])\n",
    "print(f\"Number of workflow records after filtering : {df.shape[0]}\")"
   ],
   "id": "cc818992c800ca4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2595399\n",
      "Number of deleted workflows records : 61009\n",
      "Number of workflow records after filtering : 2534390\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extracting information from a single workflow file\n",
   "id": "3c4311e9fa7227dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "firstWorkflow = df.iloc[0]\n",
    "file_hash = firstWorkflow['file_hash']\n",
    "\n",
    "folder_path = f\"../dataset/workflows\"\n",
    "file_path = os.path.join(folder_path, file_hash)"
   ],
   "id": "5b6a10bb2fba1fb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not os.path.isfile(file_path):\n",
    "    print(f\"File {file_path} not found.\")\n",
    "else:\n",
    "    parsed_data = parse_workflow(file_path)\n",
    "\n",
    "    if parsed_data:\n",
    "        print(f\"Workflow analysé : {parsed_data['file_path']}\")\n",
    "        print(f\"- Nombre de lignes : {parsed_data['lines_count']}\")\n",
    "        print(f\"- Déclencheurs : {parsed_data['events']}\")\n",
    "        print(f\"- Nombre de jobs : {parsed_data['jobs_count']}\")\n",
    "\n",
    "        for job, details in parsed_data[\"jobs\"].items():\n",
    "            print(f\"  * Job: {job}\")\n",
    "            print(f\"    - Nombre de steps: {details['steps_count']}\")\n",
    "            print(f\"    - Utilise GitHub Actions: {details['uses_github_actions']}\")\n",
    "            print(f\"    - Utilise des commandes shell: {details['uses_commands']}\")\n",
    "            print(f\"    - Détails des steps :\")\n",
    "            for step in details[\"step_details\"]:\n",
    "                print(f\"      - {step['name']}: uses={step['uses']}, run={step['run']}\")"
   ],
   "id": "546e4974f0abe647",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Filter the invalid workflows\n",
    "1. First idea is to keep only the valid workflows.\n",
    "3. Second idea is to delete all records of each uid that has at least one invalid workflow (valid_yaml = False)."
   ],
   "id": "26e9bfc0602886c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T17:03:28.625933Z",
     "start_time": "2025-03-31T17:03:27.971162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "invalid_yaml = df.loc[df['valid_yaml'] == False]\n",
    "print(f\"Number of invalid records : {len(invalid_yaml)}\\n\")\n",
    "\n",
    "print(f\"Number of records in the dataframe before : {df.shape[0]}\\n\")\n",
    "filtered_df1 = delete_invalid_yaml_records(df)\n",
    "\n",
    "print(f\"Number of records in the dataframe once filtered {filtered_df1.shape[0]}\")"
   ],
   "id": "cc8a423e967fcc6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid records : 15417\n",
      "\n",
      "Number of records in the dataframe before : 2534390\n",
      "\n",
      "Number of records in the dataframe once filtered 2518973\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T17:03:37.827458Z",
     "start_time": "2025-03-31T17:03:36.353578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "invalid_uids = df.loc[df['valid_yaml'] == False, 'uid'].unique()\n",
    "print(f\"Number of invalid uids : {len(invalid_uids)}\")\n",
    "print(f\"Number of records that corresponds to these invalids uids : {df['uid'].isin(invalid_uids).sum()}\\n\")\n",
    "\n",
    "print(f\"Number of records in the dataframe before : {df.shape[0]}\")\n",
    "print(f\"Total number of uids before filtering : {df['uid'].nunique()}\\n\")\n",
    "\n",
    "filtered_df2 = delete_uid_with_invalid_yaml(df)\n",
    "print(f\"Number of records in the dataframe once {filtered_df2.shape[0]}\")\n",
    "print(f\"Total number of uids after filtering : {filtered_df2['uid'].nunique()}\")"
   ],
   "id": "b74e2ba5084f4a5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid uids : 8384\n",
      "Number of records that corresponds to these invalids uids : 255643\n",
      "\n",
      "Number of records in the dataframe before : 2534390\n",
      "Total number of uids before filtering : 219460\n",
      "\n",
      "Number of records in the dataframe once 2278747\n",
      "Total number of uids after filtering : 211076\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Take a snapshot of the dataset\n",
    "- Filter the dataset to keep only the repositories that were committed before the snapshot date with a limit of 1 month before the snapshot date.\n",
    "- For each uid in the snapshot, get the most recent workflow."
   ],
   "id": "198b0806d44f64f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T17:04:03.424210Z",
     "start_time": "2025-03-31T17:04:00.116425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "year = 2019\n",
    "while year <= 2024:\n",
    "    # Snapshot with the first method of filtering\n",
    "    snapshot1 = extract_snapshot(filtered_df1, year)\n",
    "    # Snapshot with the second method of filtering\n",
    "    snapshot2 = extract_snapshot(filtered_df2, year)\n",
    "    # Get the most recent workflows for each uid\n",
    "    most_recent_workflows1 = get_most_recent_workflows(snapshot1, year)\n",
    "    most_recent_workflows2 = get_most_recent_workflows(snapshot2, year)\n",
    "    print(f\"Méthode 1 {year}:\")\n",
    "    print(f\"Nombre de workflows dans le snapshot: {snapshot1.shape[0]}\")\n",
    "    print(f\"Nombre de workflows les plus récents: {most_recent_workflows1.shape[0]}\\n\")\n",
    "    print(f\"Méthode 2 {year}:\")\n",
    "    print(f\"Nombre de workflows dans le snapshot: {snapshot2.shape[0]}\")\n",
    "    print(f\"Nombre de workflows les plus récents: {most_recent_workflows2.shape[0]}\\n\")\n",
    "    year += 1"
   ],
   "id": "4b68e5f8698e67f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Méthode 1 2019:\n",
      "Nombre de workflows dans le snapshot: 3965\n",
      "Nombre de workflows les plus récents: 1377\n",
      "\n",
      "Méthode 2 2019:\n",
      "Nombre de workflows dans le snapshot: 3249\n",
      "Nombre de workflows les plus récents: 1252\n",
      "\n",
      "Méthode 1 2020:\n",
      "Nombre de workflows dans le snapshot: 18902\n",
      "Nombre de workflows les plus récents: 8657\n",
      "\n",
      "Méthode 2 2020:\n",
      "Nombre de workflows dans le snapshot: 16372\n",
      "Nombre de workflows les plus récents: 7990\n",
      "\n",
      "Méthode 1 2021:\n",
      "Nombre de workflows dans le snapshot: 32199\n",
      "Nombre de workflows les plus récents: 15920\n",
      "\n",
      "Méthode 2 2021:\n",
      "Nombre de workflows dans le snapshot: 28453\n",
      "Nombre de workflows les plus récents: 14762\n",
      "\n",
      "Méthode 1 2022:\n",
      "Nombre de workflows dans le snapshot: 44983\n",
      "Nombre de workflows les plus récents: 21713\n",
      "\n",
      "Méthode 2 2022:\n",
      "Nombre de workflows dans le snapshot: 40612\n",
      "Nombre de workflows les plus récents: 20342\n",
      "\n",
      "Méthode 1 2023:\n",
      "Nombre de workflows dans le snapshot: 74622\n",
      "Nombre de workflows les plus récents: 35557\n",
      "\n",
      "Méthode 2 2023:\n",
      "Nombre de workflows dans le snapshot: 69312\n",
      "Nombre de workflows les plus récents: 33804\n",
      "\n",
      "Méthode 1 2024:\n",
      "Nombre de workflows dans le snapshot: 65493\n",
      "Nombre de workflows les plus récents: 30281\n",
      "\n",
      "Méthode 2 2024:\n",
      "Nombre de workflows dans le snapshot: 61018\n",
      "Nombre de workflows les plus récents: 28918\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
