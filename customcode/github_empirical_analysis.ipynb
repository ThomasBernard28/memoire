{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-24T18:58:10.059543Z",
     "start_time": "2025-03-24T18:58:09.770532Z"
    }
   },
   "source": [
    "from parser import *\n",
    "from snapshotExtraction import extract_snapshot, get_most_recent_workflows\n",
    "import pandas as pd\n",
    "import os\n",
    "from ruamel.yaml import YAML"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extracting information from a single workflow file\n",
   "id": "3c4311e9fa7227dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T19:00:34.896844Z",
     "start_time": "2025-03-24T19:00:24.436074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('../dataset/200_workflowsonly.csv')\n",
    "print(df.shape[0])\n",
    "\n",
    "firstWorkflow = df.iloc[0]\n",
    "file_hash = firstWorkflow['file_hash']\n",
    "\n",
    "folder_path = f\"../dataset/workflows\"\n",
    "file_path = os.path.join(folder_path, file_hash)"
   ],
   "id": "5b6a10bb2fba1fb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2595399\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T18:48:43.348390Z",
     "start_time": "2025-03-24T18:48:43.339992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.isfile(file_path):\n",
    "    print(f\"File {file_path} not found.\")\n",
    "else:\n",
    "    parsed_data = parse_workflow(file_path)\n",
    "\n",
    "    if parsed_data:\n",
    "        print(f\"Workflow analysé : {parsed_data['file_path']}\")\n",
    "        print(f\"- Nombre de lignes : {parsed_data['lines_count']}\")\n",
    "        print(f\"- Déclencheurs : {parsed_data['events']}\")\n",
    "        print(f\"- Nombre de jobs : {parsed_data['jobs_count']}\")\n",
    "\n",
    "        for job, details in parsed_data[\"jobs\"].items():\n",
    "            print(f\"  * Job: {job}\")\n",
    "            print(f\"    - Nombre de steps: {details['steps_count']}\")\n",
    "            print(f\"    - Utilise GitHub Actions: {details['uses_github_actions']}\")\n",
    "            print(f\"    - Utilise des commandes shell: {details['uses_commands']}\")\n",
    "            print(f\"    - Détails des steps :\")\n",
    "            for step in details[\"step_details\"]:\n",
    "                print(f\"      - {step['name']}: uses={step['uses']}, run={step['run']}\")"
   ],
   "id": "546e4974f0abe647",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow analysé : ../dataset/workflows\\6f1b3e145aeb96e714212eb0b5e05088d7d6b50a4a945a3488779a94a85c80f0\n",
      "- Nombre de lignes : 27\n",
      "- Déclencheurs : ['push', 'pull_request']\n",
      "- Nombre de jobs : 1\n",
      "  * Job: build\n",
      "    - Nombre de steps: 6\n",
      "    - Utilise GitHub Actions: True\n",
      "    - Utilise des commandes shell: True\n",
      "    - Détails des steps :\n",
      "      - Checkout code: uses=actions/checkout@v3, run=None\n",
      "      - Setup node: uses=actions/setup-node@v3, run=None\n",
      "      - Install dependencies: uses=None, run=if ! yarn install; then\n",
      "  cat /tmp/xfs-*/buildfile.log 2>/dev/null || true\n",
      "  exit 1\n",
      "fi\n",
      "\n",
      "      - Lint: uses=None, run=yarn lint\n",
      "      - Check types: uses=None, run=yarn types:check\n",
      "      - Build: uses=None, run=yarn build:chrome\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Take a snapshot of the dataset\n",
    "- Filter the dataset to keep only the repositories that were committed before the snapshot date with a limit of 1 month before the snapshot date.\n",
    "- For each uid in the snapshot, get the most recent workflow."
   ],
   "id": "198b0806d44f64f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T19:11:28.872708Z",
     "start_time": "2025-03-24T19:11:28.551753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "snapshot = extract_snapshot(df, 2024)\n",
    "most_recent_workflows = get_most_recent_workflows(snapshot, 2024)\n",
    "print(f\"Nombre de workflows dans le snapshot : {snapshot.shape[0]}\")\n",
    "print(f\"Nombre de workflows les plus récents : {most_recent_workflows.shape[0]}\")"
   ],
   "id": "4b68e5f8698e67f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de workflows dans le snapshot : 67013\n",
      "Nombre de workflows les plus récents : 31230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\PycharmProjects\\memoire\\customcode\\snapshotExtraction.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  snapshot['days_from_date'] = (snapshot['committed_date'] - snapshot_date).abs()\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
