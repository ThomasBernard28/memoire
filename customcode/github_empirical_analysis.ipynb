{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T10:09:44.925994Z",
     "start_time": "2025-03-25T10:09:44.923148Z"
    }
   },
   "source": [
    "from parser import *\n",
    "from snapshoter import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from ruamel.yaml import YAML"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the dataset and filtering the deleted workflows",
   "id": "d0f477814f578a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T09:30:56.628479Z",
     "start_time": "2025-03-25T09:30:45.813558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('../dataset/200_workflowsonly.csv')\n",
    "print(df.shape[0])\n",
    "\n",
    "deleted_workflows = df.loc[df['file_hash'].isnull()]\n",
    "print(f\"Number of deleted workflows records : {len(deleted_workflows)}\")\n",
    "\n",
    "df = df.dropna(subset=['file_hash'])\n",
    "print(f\"Number of workflow records after filtering : {df.shape[0]}\")"
   ],
   "id": "cc818992c800ca4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2595399\n",
      "Number of deleted workflows : 61009\n",
      "Number of workflows after filtering : 2534390\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extracting information from a single workflow file\n",
   "id": "3c4311e9fa7227dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T09:34:27.404710Z",
     "start_time": "2025-03-25T09:34:27.401439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "firstWorkflow = df.iloc[0]\n",
    "file_hash = firstWorkflow['file_hash']\n",
    "\n",
    "folder_path = f\"../dataset/workflows\"\n",
    "file_path = os.path.join(folder_path, file_hash)"
   ],
   "id": "5b6a10bb2fba1fb5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not os.path.isfile(file_path):\n",
    "    print(f\"File {file_path} not found.\")\n",
    "else:\n",
    "    parsed_data = parse_workflow(file_path)\n",
    "\n",
    "    if parsed_data:\n",
    "        print(f\"Workflow analysé : {parsed_data['file_path']}\")\n",
    "        print(f\"- Nombre de lignes : {parsed_data['lines_count']}\")\n",
    "        print(f\"- Déclencheurs : {parsed_data['events']}\")\n",
    "        print(f\"- Nombre de jobs : {parsed_data['jobs_count']}\")\n",
    "\n",
    "        for job, details in parsed_data[\"jobs\"].items():\n",
    "            print(f\"  * Job: {job}\")\n",
    "            print(f\"    - Nombre de steps: {details['steps_count']}\")\n",
    "            print(f\"    - Utilise GitHub Actions: {details['uses_github_actions']}\")\n",
    "            print(f\"    - Utilise des commandes shell: {details['uses_commands']}\")\n",
    "            print(f\"    - Détails des steps :\")\n",
    "            for step in details[\"step_details\"]:\n",
    "                print(f\"      - {step['name']}: uses={step['uses']}, run={step['run']}\")"
   ],
   "id": "546e4974f0abe647",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Filter the invalid workflows\n",
    "1. First idea is to deleted all records of each uid that has at least one invalid workflow (valid_yaml = False).\n",
    "2. Second idea is to keep only the valid workflows."
   ],
   "id": "26e9bfc0602886c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T10:18:48.814118Z",
     "start_time": "2025-03-25T10:18:47.390397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "invalid_uids = df.loc[df['valid_yaml'] == False, 'uid'].unique()\n",
    "print(f\"Number of invalid uids : {len(invalid_uids)}\")\n",
    "print(f\"Number of records that corresponds to these invalids uids : {df['uid'].isin(invalid_uids).sum()}\\n\")\n",
    "\n",
    "print(f\"Number of records in the dataframe before : {df.shape[0]}\")\n",
    "print(f\"Total number of uids before filtering : {df['uid'].nunique()}\\n\")\n",
    "\n",
    "filtered_df = delete_uid_with_invalid_yaml(df)\n",
    "print(f\"Number of records in the dataframe once {filtered_df.shape[0]}\")\n",
    "print(f\"Total number of uids after filtering : {filtered_df['uid'].nunique()}\")"
   ],
   "id": "b74e2ba5084f4a5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid uids : 8384\n",
      "Number of records that corresponds to these invalids uids : 255643\n",
      "\n",
      "Number of records in the dataframe before : 2534390\n",
      "Total number of uids before filtering : 219460\n",
      "\n",
      "Number of records in the dataframe once 2278747\n",
      "Total number of uids after filtering : 211076\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T10:21:54.064338Z",
     "start_time": "2025-03-25T10:21:53.464615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "invalid_yaml = df.loc[df['valid_yaml'] == False]\n",
    "print(f\"Number of invalid records : {len(invalid_yaml)}\\n\")\n",
    "\n",
    "print(f\"Number of records in the dataframe before : {df.shape[0]}\\n\")\n",
    "filtered_df = delete_invalid_yaml_records(df)\n",
    "\n",
    "print(f\"Number of records in the dataframe once filtered {filtered_df.shape[0]}\")\n"
   ],
   "id": "bcb7526306edb0c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid records : 15417\n",
      "\n",
      "Number of records in the dataframe before : 2534390\n",
      "\n",
      "Number of records in the dataframe once filtered 2518973\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Take a snapshot of the dataset\n",
    "- Filter the dataset to keep only the repositories that were committed before the snapshot date with a limit of 1 month before the snapshot date.\n",
    "- For each uid in the snapshot, get the most recent workflow."
   ],
   "id": "198b0806d44f64f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T10:15:58.085949Z",
     "start_time": "2025-03-25T10:15:57.804829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "snapshot = extract_snapshot(df, 2019)\n",
    "most_recent_workflows = get_most_recent_workflows(snapshot, 2019)\n",
    "print(f\"Nombre de workflows dans le snapshot : {snapshot.shape[0]}\")\n",
    "print(f\"Nombre de workflows les plus récents : {most_recent_workflows.shape[0]}\")"
   ],
   "id": "4b68e5f8698e67f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de workflows dans le snapshot : 4067\n",
      "Nombre de workflows les plus récents : 1380\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
